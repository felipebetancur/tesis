\subsection{Aprendizaje automático}

	En los últimos años, con el crecimiento exponencial de la información hemos entrado en la era de Big Data o grandes datos. Para dar un idea del concepto, por ejemplo, podemos tener en cuenta la cantidad de horas de video que se suben por minuto al sitio YouTube que según las últimas estadísticas~\cite{YoutubeStats} rondan las 100 horas por minuto. El sitio acumula en promedio un total de 10 años de contenido por día con lo cual se puede ver claramente la necesidad, por ejemplo, de filtrar y bloquear el contenido restringido por copyright de millones de videos. Otro caso de mayor escala lo podemos encontrar en la empresa Google, en el buscador de esta empresa, se realizan en promedio 5 mil millones de búsquedas diarias~\cite{GoogleSearches}. Lo impresionante de este último caso es la cantidad de sitios web creados sobre los cuales el buscador de Google tiene que trabajar, estadísticas recientes~\cite{Websites}, aseguran que hay aproximadamente 1 trillón de sitios web. Se puede ver que Google para poder procesar esa cantidad de información incalculable necesita de la automatización de miles de procesos.
	
	 Debido a esto, es imperativo tener algún método automático que nos ayude a clasificar y analizar toda esta información ya que sobrepasa la capacidad de las personas de hacerlo por sí mismas. De ahí surge el campo de \textit{aprendizaje automático} o \textit{machine learning} para proveer métodos que resuelvan estos problemas. En particular, se define al aprendizaje automático como un conjunto de métodos que pueden detectar automáticamente patrones en los datos y luego usar esos patrones descubiertos para predecir datos futuros o poder tomar ciertas decisiones en condiciones de incertidumbre.
	 
	 El campo de \textit{machine learning}, tiene fuertes bases en la estadística por lo cual, conceptos como los desarrollados en el capítulo de ``Conceptos Preeliminares'' van a ser de utilidad para comprender las siguientes subsecciones. Este campo, tiene dos grandes areas que vamos a ver a continuación: el aprendizaje supervisado y el no supervisado. Si bien existen tareas de \textit{machine learning} que pueden ser categorizadas en otras areas, las que se acaban de nombrar son las más destacadas.
	
	\input{capitulo2/probability_concepts.tex}

	
	\subsubsection{Aprendizaje supervisado}
	
	El aprendizaje supervisado es una rama del aprendizaje automático cuyo objetivo es, dado un conjunto de entradas denotado por $x$ y uno de salidas denotado por $y$, establecer un mapeo entre $x$ e $y$ dado un conjunto etiquetado de pares de entrada-salida $M=\{(x_i,y_i)\}^{N}_{i=1}$ donde $M$ es llamado el \textit{conjunto de entrenamiento} y $N$ es el número de ejemplos de entrenamiento.
	
	El conjunto de entrenamiento es un elemento indispensable en cualquier algoritmo de aprendizaje supervisado, ya que representa la base fundamental de conocimiento necesaria para que algoritmo pueda realizar futuras predicciones. En general, mientras más conocmiento se tenga sobre las características del objeto de interés que se esté analizando, más precisa va a ser la clasificación sobre nuevas entradas. Este conocimiento se construye a partir de la variabilidad de los objetos conocidos, es decir el poder contar con un gran conjunto de muestras que reflejen las posibles variaciones del objeto de interés. Esto le otorga robustez a la clasificación.
	
	En la configuración más simple, cada entrada $x_i$ del conjunto de entrenamiento es un vector $D$-dimensional de números. Estas son llamadas \textit{características} (\textit{features}, de su traducción al inglés). En general, sin embargo, $x_i$ puede ser un objeto con una estructura compleja, como una imagen, un mensaje de correo, etc.
	
	Dependiendo del tipo de problema a tratar, la salida $y_i$ puede ser una variable categórica, donde $y_i \in \{1,\dots,C\}$ (conjunto finito clases), o puede ser un valor real. Cuando $y_i$ es una variable categórica, estamos frente a un problema de \textit{clasificación} donde el objetivo es ``etiquetar'' o nombrar los objetos observados. Cuando $y_i$ es una variable real, estamos en presencia de un problema de regresión donde el objetivo es predecir una variable continua.
	
	
	\paragraph{Clasificación}  ~\\
	
		Como se explicó anteriormente, el objetivo del aprendizaje supervisado es aprender un mapeo desde las entradas $x$ a las salidas $y$, donde $y_i \in \{1,\dots,C\}$ con $C$ siendo el número de clases. Dependiendo de la cantidad de clases, podemos encontrarnos con distintos tipos de clasificaciones. Si $C=2$, estamos ante una \textit{clasificación binaria} (en la cual asumimos que $y\in\{0,1\}$); mientras que si $C>2$, la clasificación pasa a ser \textit{multiclase}. Existe otro tipo de clasificación denominada \textit{clasificación multi-etiqueta}, que difiere de la multiclase en cuanto a que las clases no son mutuamente excluyentes, es decir, una muestra puede pertenecer a dos o más categorías o clases. En este último caso, el mapeo se realiza desde la entrada $x$ a un vector $z$, más que a una salida escalar. La elección de cual usar esta directamente asociada al tipo de problema que se quiera resolver.
		
		Una manera de formalizar el problema es a partir de una función de aproximación. Se asume $y = f(x)$ para cierta función desconocida $f$, y el objetivo del apredizaje es estimar la función $f$ dado un conjunto de entrenamiento etiquetado. Posteriormente, se realizan predicciones usando $\hat{y} = \hat{f}(x)$ (usamos el símbolo \string^ para denotar estimación). El objetivo principal es realizar predicciones en entradas nuevas, es decir, que no se han visto antes, a esto se le llama \textit{generalización}.
		
		Tal como expresa P. Domingos en \cite{PDomingo} el objetivo del aprendizaje automático es \textit{generalizar} más allá de los ejemplos en el conjunto de entrenamiento. Esto es porque, no importa cuantos datos tengamos, es muy poco probable que vayamos a ver los mismos ejemplos al momento de evaluar.
		
	\paragraph{Clasificadores probabilísticos} ~\\

		Los clasificadores probabilísticos son una rama de los clasificadores que hacen uso de la inferencia estadística para encontrar la mejor clase dada una muestra. A diferencia de otros clasificadores que simplemente dan como salida la mejor clase, los clasificadores probabilísticos muestran en su salida la probabilidad de que la muestra sea miembro de cada una de las clases posibles.


	\paragraph{Regresión} ~\\
	
		\RC{Reformular}
	
		La regresión es como la clasificación excepto que la respuesta es una variable continua. Ejemplos de problemas reales de regresión:
		\begin{itemize}
			\item Predecir el precio de mañana del mercado de stock dado las condiciones actuales de mercado y dada cualquier otra información relevante.
			\item Predecir la edad de un espectador que mira un video de YouTube.
			\item Predecir la temperatura en cualquier ubicación de una construcción usando el clima, el tiempo, etc.
		\end{itemize}
	
	
	\subsubsection{Aprendizaje no supervisado}
	
		\RC{Reformular}
		
		El aprendizaje no supervisado es la otra rama del aprendizaje automático cuyo objetivo es encontrar ``estructuras interesantes'' en los datos. A diferencia del aprendizaje supervisado, no se establece qué salida se tiene que dar para cada entrada. En cambio, se busca construir modelos de la forma $p(x_i | \theta)$ donde $\theta$ es un vector de parámetros y $x_i$ es un dato de entada. Hay dos diferencias con el aprendizaje supervisado. La primera, es que hemos establecido $p(x_i | \theta)$ en vez de $p(x_i | y_i, \theta)$; es decir, el aprendizaje supervisado es una estimación condicional, mientras que el aprendizaje no supervisado es una estimación no condicional. La segunda, $x_i$ es un vector de características, por lo que se requiere crear modelos de probabilidad multivariable. Por el contrario, en el aprendizaje supervisado, $y_i$ es usualmente una simple variable que se está intentado predecir.
		
		El aprendizaje no supervisado no requiere de que haya una persona que etiquete los datos manualmente, lo cual no solo es costoso, sino que además contiene relativamente poca información, sin duda no es suficiente para estimar de forma fiable los parámetros en modelos más complejos.