\subsubsection{Clasificador Na\"{i}ve Bayes}

	En \cite{NarasMurty}, Narasimha y Susheela definen a Na\"{i}ve Bayes como un clasificador probabilístico basado en la aplicación del Teorema de Bayes con una fuerte suposición de independencia, na\"{i}ve. En términos simples, los autores detallan que un clasificador Na\"{i}ve Bayes asume que la presencia o ausencia de una característica particular no está relacionada con la presencia o ausencia de cualquier otra característica. Este tipo de clasificador considera que cada una de estas características contribuye independientemente a la probabilidad de que un elemento sea de una clase particular, independientemente de la presencia o ausencia de otras características. Por ejemplo, una fruta puede ser considerada una manzana si es roja, redonda y de 7cm de diámetro aproximadamente.

	El modelo general para un clasificador es:
		$$p(C \vert F_1,\dots,F_n)$$
	sobre una variable dependiente C, con un pequeño número de resultados, o clases. Esta variable está condicionada por varias variables dependientes, \textit{features}, desde $F_1$ a $F_n$ \cite{NarasMurty}. El problema es que si el número $n$ de variables dependientes es grande, o cuando éstas pueden tomar muchos valores, entonces basar este modelo en tablas de probabilidad se vuelve computacionalmente imposible. Por ejemplo, si hubiesen $35$ variables independientes con $2$ valores posibles cada una entonces habrían $34.359.738.368$ valores posibles distribuidos en múltiples tablas. Si se usaran decimales de punto flotante simple (4 bytes), se necesitarían 32 gigabytes de memoria para almacenar todo, lo que lo hace muy poco manipulable. Por lo tanto, Narasimha et. al en \cite{NarasMurty}, reformulan el modelo para hacerlo más manejable:
Usando el teorema de Bayes se escribe:
		\begin{align*}
		p(C \vert F_1,\dots,F_n) = \frac{p(C) \ p(F_1,\dots,F_n\vert C)}{p(F_1,\dots,F_n)}.
		\end{align*}
		que puede ser reescrita como sigue, aplicando repetidamente la definición de probabilidad condicional:
		\begin{align}
		p(C, F_1, \dots, F_n)
		&= p(C) \ p(F_1,\dots,F_n\vert C) \\
		&= p(C) \ p(F_1\vert C) \ p(F_2,\dots,F_n\vert C, F_1) \\
		&= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3,\dots,F_n\vert C, F_1, F_2)
		\end{align}
		y así sucesivamente. Ahora es cuando la asunción "na\"{i}ve" de independencia condicional entra en juego: se asume que cada $F_i$ es independiente de cualquier otra $F_j$ para $j \neq i$. Esto significa que
		\begin{align*}
		p(F_i \vert C, F_j) = p(F_i \vert C)
		\end{align*}
		por lo que la probabilidad compuesta puede expresarse como
		\begin{align*}
		p(C, F_1, \dots, F_n) 
		&= p(C) \ p(F_1\vert C) \ p(F_2\vert C) \ p(F_3\vert C) \dots \\
		&= p(C) \prod_{i=1}^n p(F_i \vert C).
		\end{align*}
		Esto significa que haciendo estas presunciones, la distribución condicional sobre C puede expresarse de la siguiente manera:
		$$p(C \vert F_1,\dots,F_n) = \frac{1}{Z}p(C)\prod_{i=1}^n p(F_i \vert C)$$
		donde $Z$ es un factor que depende sólo de $F_1,\dots , F_n$, es decir, constante si los valores de $F_i$ son conocidos \cite{NarasMurty}.
		
		La siguiente subsección, si bien no está relacionada directamente con Na\"{i}ve Bayes, va a ser de ayuda para entender al clasificador \textit{Random Forest}. Lo que se ha visto hasta ahora sobre Na\"{i}ve Bayes junto con lo se va a ver en la sección \ref{subsubsection:random_forest} sobre Random Forest va a servir para comprender al clasificador Random Fern.
		